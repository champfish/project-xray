{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d66f185a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#Needed for deterministic behavior\n",
    "#os.environ[\"CUBLAS_WORKSPACE_CONFIG\"]= \":16:8\"\n",
    "#os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "\n",
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2750cbdc",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1002419874.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[4], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    import pytorch-fid\u001b[0m\n\u001b[1;37m                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "#import pytorch-fid\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from pathlib import Path\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "from tqdm import tqdm\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import os \n",
    "import sys\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ece07baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPGenerator(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 number_of_hidden_layers: int, \n",
    "                 input_size: int, \n",
    "                 hidden_size: int, \n",
    "                 output_size: int,\n",
    "                 activation: torch.nn.Module):\n",
    "        \"\"\"Construct a simple MLP generator\"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        self.label_emb = torch.nn.Embedding(2,2)\n",
    "        assert number_of_hidden_layers >= 0, \"Generator number_of_hidden_layers must be at least 0\"\n",
    "        \n",
    "        dims_in = [input_size] + [hidden_size] * number_of_hidden_layers\n",
    "        # final output should be the size of a true example\n",
    "        dims_out = [hidden_size] * number_of_hidden_layers + [output_size] \n",
    "        layers = []\n",
    "        for i in range(number_of_hidden_layers + 1):\n",
    "            layers.append(torch.nn.Linear(dims_in[i], dims_out[i]))\n",
    "            \n",
    "            if i < number_of_hidden_layers:\n",
    "                layers.append(activation)\n",
    "        \n",
    "        # apply Sigmoid after final layer to constrain generated images to [0, 1]\n",
    "        layers.append(torch.nn.Sigmoid())\n",
    "        \n",
    "        self.net = torch.nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, labels):\n",
    "        c = self.label_emb(labels)\n",
    "        xx = torch.cat([z,c],1)\n",
    "        return self.net(xx)\n",
    "\n",
    "class MLPDiscriminator(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 number_of_hidden_layers: int, \n",
    "                 input_size: int, \n",
    "                 hidden_size: int, \n",
    "                 activation: torch.nn.Module):\n",
    "        \"\"\"Construct a simple MLP discriminator\"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        self.label_emb = torch.nn.Embedding(2,2)\n",
    "        msg = \"Discriminator number_of_hidden_layers must be at least 0\"\n",
    "        assert number_of_hidden_layers >= 0, msg\n",
    "        \n",
    "        # final output dimension is scalar (probability image is real)\n",
    "        dims_in = [input_size] + [hidden_size] * number_of_hidden_layers\n",
    "        dims_out = [hidden_size] * number_of_hidden_layers + [1]\n",
    "        \n",
    "        layers = []\n",
    "        for i in range(number_of_hidden_layers + 1):\n",
    "            layers.append(torch.nn.Linear(dims_in[i], dims_out[i]))\n",
    "            \n",
    "            if i < number_of_hidden_layers:\n",
    "                layers.append(activation)\n",
    "        \n",
    "        # apply sigmoid after final layer to represent probability\n",
    "        layers.append(torch.nn.Sigmoid())\n",
    "        \n",
    "        self.net = torch.nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, labels):\n",
    "        c = self.label_emb(labels)\n",
    "        xx = torch.cat([x,c],1)\n",
    "        return self.net(xx)\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 number_of_hidden_layers: int, \n",
    "                 latent_size: int, \n",
    "                 hidden_size: int, \n",
    "                 output_size: int,\n",
    "                 activation_generator: torch.nn.Module = torch.nn.ReLU(),\n",
    "                 activation_discriminator: torch.nn.Module = torch.nn.LeakyReLU(0.2)\n",
    "                ):\n",
    "        \n",
    "        super().__init__()\n",
    "                          \n",
    "        self.generator = MLPGenerator(\n",
    "            number_of_hidden_layers=number_of_hidden_layers, \n",
    "            input_size=latent_size+2, \n",
    "            hidden_size=hidden_size, \n",
    "            output_size=output_size,\n",
    "            activation=activation_generator\n",
    "        )\n",
    "        \n",
    "        self.discriminator = MLPDiscriminator(\n",
    "            number_of_hidden_layers=number_of_hidden_layers, \n",
    "            input_size=output_size+2, \n",
    "            hidden_size=hidden_size,\n",
    "            activation=activation_discriminator\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "391d9d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(batch_size: int = 128):\n",
    "    \"\"\"\n",
    "    Load dataset.\n",
    "    :return: DataLoader object\n",
    "    \"\"\"\n",
    "\n",
    "    cuda_kwargs = {\n",
    "        'num_workers': 1,\n",
    "        'pin_memory': True,\n",
    "        'shuffle': True\n",
    "    } if torch.cuda.is_available() else {}\n",
    "\n",
    "    # format image data, but do not normalize\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.Grayscale(num_output_channels=1),\n",
    "         transforms.ToTensor()])\n",
    "    path = \"image_data/cleaned_train\"\n",
    "    dataset = datasets.ImageFolder(path,transform)\n",
    "    # load data\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        **cuda_kwargs\n",
    "    )\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7138e62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is cuda\n"
     ]
    }
   ],
   "source": [
    "# Training hyperparameters that you shouldn't need to change.\n",
    "# Consider changing batch size only if you are trying to speed up your training.\n",
    "image_size = 128 #28\n",
    "batch_size = 64\n",
    "\n",
    "# select device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device is\",device)\n",
    "\n",
    "# load MNIST dataset\n",
    "mnist = load_data(batch_size=batch_size)\n",
    "\n",
    "# discriminator loss function: binary cross-entropy loss\n",
    "discrim_loss_func = torch.nn.BCELoss()\n",
    "\n",
    "# determine which labels will correspond to \"real\" and \"fake\" predictions from the discriminator\n",
    "label_real = 1.0\n",
    "label_fake = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d7acb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Training hyperparameters\n",
    "    hidden_size = 4096\n",
    "    number_of_hidden_layers = 2\n",
    "    learning_rate = 0.00001\n",
    "    epochs = 150\n",
    "    latent_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abec419f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training!\n",
      "Epoch:    1  Gen Loss:      1.2  Dis Loss:      1.0  Total:      2.3  Time: 00:00:08\n",
      "Epoch:    2  Gen Loss:      1.6  Dis Loss:      1.0  Total:      2.6  Time: 00:00:14\n",
      "Epoch:    3  Gen Loss:      1.7  Dis Loss:      1.3  Total:      3.0  Time: 00:00:20\n",
      "Epoch:    4  Gen Loss:      1.8  Dis Loss:      1.0  Total:      2.8  Time: 00:00:26\n",
      "Epoch:    5  Gen Loss:      2.2  Dis Loss:      0.8  Total:      3.1  Time: 00:00:33\n",
      "Epoch:    6  Gen Loss:      2.2  Dis Loss:      0.8  Total:      3.0  Time: 00:00:39\n",
      "Epoch:    7  Gen Loss:      2.8  Dis Loss:      0.7  Total:      3.5  Time: 00:00:45\n",
      "Epoch:    8  Gen Loss:      3.4  Dis Loss:      0.2  Total:      3.6  Time: 00:00:52\n",
      "Epoch:    9  Gen Loss:      2.9  Dis Loss:      0.3  Total:      3.2  Time: 00:00:58\n",
      "Epoch:   10  Gen Loss:      2.2  Dis Loss:      0.6  Total:      2.7  Time: 00:01:04\n",
      "Epoch:   11  Gen Loss:      1.6  Dis Loss:      1.1  Total:      2.7  Time: 00:01:10\n",
      "Epoch:   12  Gen Loss:      2.0  Dis Loss:      1.0  Total:      3.0  Time: 00:01:16\n",
      "Epoch:   13  Gen Loss:      2.6  Dis Loss:      1.1  Total:      3.7  Time: 00:01:22\n",
      "Epoch:   14  Gen Loss:      2.8  Dis Loss:      0.7  Total:      3.4  Time: 00:01:29\n",
      "Epoch:   15  Gen Loss:      2.4  Dis Loss:      0.8  Total:      3.2  Time: 00:01:35\n",
      "Epoch:   16  Gen Loss:      2.0  Dis Loss:      1.0  Total:      3.0  Time: 00:01:41\n",
      "Epoch:   17  Gen Loss:      1.9  Dis Loss:      1.0  Total:      2.9  Time: 00:01:47\n",
      "Epoch:   18  Gen Loss:      2.1  Dis Loss:      1.2  Total:      3.3  Time: 00:01:53\n",
      "Epoch:   19  Gen Loss:      2.2  Dis Loss:      1.3  Total:      3.5  Time: 00:02:00\n",
      "Epoch:   20  Gen Loss:      1.1  Dis Loss:      1.6  Total:      2.7  Time: 00:02:06\n",
      "Epoch:   21  Gen Loss:      1.5  Dis Loss:      1.4  Total:      2.9  Time: 00:02:12\n",
      "Epoch:   22  Gen Loss:      3.6  Dis Loss:      1.1  Total:      4.7  Time: 00:02:18\n",
      "Epoch:   23  Gen Loss:      2.4  Dis Loss:      0.9  Total:      3.3  Time: 00:02:24\n",
      "Epoch:   24  Gen Loss:      2.2  Dis Loss:      0.9  Total:      3.1  Time: 00:02:30\n",
      "Epoch:   25  Gen Loss:      2.8  Dis Loss:      1.2  Total:      4.0  Time: 00:02:37\n",
      "Epoch:   26  Gen Loss:      1.6  Dis Loss:      1.6  Total:      3.2  Time: 00:02:43\n",
      "Epoch:   27  Gen Loss:      1.9  Dis Loss:      1.3  Total:      3.2  Time: 00:02:49\n",
      "Epoch:   28  Gen Loss:      2.7  Dis Loss:      0.6  Total:      3.3  Time: 00:02:55\n",
      "Epoch:   29  Gen Loss:      1.8  Dis Loss:      1.2  Total:      3.0  Time: 00:03:02\n",
      "Epoch:   30  Gen Loss:      2.5  Dis Loss:      0.6  Total:      3.1  Time: 00:03:08\n",
      "Epoch:   31  Gen Loss:      2.8  Dis Loss:      0.8  Total:      3.6  Time: 00:03:14\n",
      "Epoch:   32  Gen Loss:      2.1  Dis Loss:      0.5  Total:      2.6  Time: 00:03:20\n",
      "Epoch:   33  Gen Loss:      2.2  Dis Loss:      1.0  Total:      3.1  Time: 00:03:26\n",
      "Epoch:   34  Gen Loss:      2.5  Dis Loss:      0.9  Total:      3.5  Time: 00:03:32\n",
      "Epoch:   35  Gen Loss:      2.3  Dis Loss:      0.5  Total:      2.9  Time: 00:03:38\n",
      "Epoch:   36  Gen Loss:      2.8  Dis Loss:      1.0  Total:      3.7  Time: 00:03:44\n",
      "Epoch:   37  Gen Loss:      2.5  Dis Loss:      0.8  Total:      3.3  Time: 00:03:51\n",
      "Epoch:   38  Gen Loss:      1.7  Dis Loss:      1.5  Total:      3.2  Time: 00:03:57\n",
      "Epoch:   39  Gen Loss:      2.5  Dis Loss:      0.8  Total:      3.2  Time: 00:04:03\n",
      "Epoch:   40  Gen Loss:      2.4  Dis Loss:      1.3  Total:      3.7  Time: 00:04:09\n",
      "Epoch:   41  Gen Loss:      1.6  Dis Loss:      1.5  Total:      3.1  Time: 00:04:15\n",
      "Epoch:   42  Gen Loss:      2.3  Dis Loss:      0.8  Total:      3.1  Time: 00:04:22\n",
      "Epoch:   43  Gen Loss:      2.8  Dis Loss:      0.9  Total:      3.7  Time: 00:04:28\n",
      "Epoch:   44  Gen Loss:      3.1  Dis Loss:      1.0  Total:      4.1  Time: 00:04:34\n",
      "Epoch:   45  Gen Loss:      2.0  Dis Loss:      0.9  Total:      2.9  Time: 00:04:41\n",
      "Epoch:   46  Gen Loss:      2.0  Dis Loss:      1.0  Total:      3.0  Time: 00:04:47\n",
      "Epoch:   47  Gen Loss:      3.0  Dis Loss:      0.4  Total:      3.4  Time: 00:04:53\n",
      "Epoch:   48  Gen Loss:      2.0  Dis Loss:      0.7  Total:      2.7  Time: 00:04:59\n",
      "Epoch:   49  Gen Loss:      2.4  Dis Loss:      0.6  Total:      3.0  Time: 00:05:05\n",
      "Epoch:   50  Gen Loss:      2.5  Dis Loss:      0.5  Total:      3.0  Time: 00:05:12\n",
      "Epoch:   51  Gen Loss:      2.6  Dis Loss:      0.7  Total:      3.3  Time: 00:05:18\n",
      "Epoch:   52  Gen Loss:      3.5  Dis Loss:      0.3  Total:      3.8  Time: 00:05:24\n",
      "Epoch:   53  Gen Loss:      3.1  Dis Loss:      0.3  Total:      3.4  Time: 00:05:30\n",
      "Epoch:   54  Gen Loss:      2.8  Dis Loss:      0.4  Total:      3.2  Time: 00:05:36\n",
      "Epoch:   55  Gen Loss:      3.3  Dis Loss:      0.2  Total:      3.5  Time: 00:05:42\n",
      "Epoch:   56  Gen Loss:      2.8  Dis Loss:      0.3  Total:      3.1  Time: 00:05:49\n",
      "Epoch:   57  Gen Loss:      2.8  Dis Loss:      0.4  Total:      3.2  Time: 00:05:55\n",
      "Epoch:   58  Gen Loss:      2.7  Dis Loss:      0.5  Total:      3.1  Time: 00:06:01\n",
      "Epoch:   59  Gen Loss:      3.3  Dis Loss:      0.4  Total:      3.6  Time: 00:06:07\n",
      "Epoch:   60  Gen Loss:      3.2  Dis Loss:      0.3  Total:      3.5  Time: 00:06:13\n",
      "Epoch:   61  Gen Loss:      3.4  Dis Loss:      0.3  Total:      3.7  Time: 00:06:19\n",
      "Epoch:   62  Gen Loss:      3.7  Dis Loss:      0.3  Total:      4.1  Time: 00:06:25\n",
      "Epoch:   63  Gen Loss:      3.2  Dis Loss:      0.3  Total:      3.6  Time: 00:06:32\n",
      "Epoch:   64  Gen Loss:      3.3  Dis Loss:      0.4  Total:      3.6  Time: 00:06:38\n",
      "Epoch:   65  Gen Loss:      3.1  Dis Loss:      0.4  Total:      3.5  Time: 00:06:44\n",
      "Epoch:   66  Gen Loss:      3.2  Dis Loss:      0.5  Total:      3.7  Time: 00:06:50\n",
      "Epoch:   67  Gen Loss:      3.5  Dis Loss:      0.4  Total:      3.9  Time: 00:06:56\n",
      "Epoch:   68  Gen Loss:      3.9  Dis Loss:      0.2  Total:      4.1  Time: 00:07:02\n",
      "Epoch:   69  Gen Loss:      4.0  Dis Loss:      0.2  Total:      4.3  Time: 00:07:08\n",
      "Epoch:   70  Gen Loss:      3.8  Dis Loss:      0.2  Total:      4.0  Time: 00:07:15\n",
      "Epoch:   71  Gen Loss:      4.0  Dis Loss:      0.1  Total:      4.1  Time: 00:07:21\n",
      "Epoch:   72  Gen Loss:      3.6  Dis Loss:      0.2  Total:      3.8  Time: 00:07:27\n",
      "Epoch:   73  Gen Loss:      3.6  Dis Loss:      0.2  Total:      3.8  Time: 00:07:33\n",
      "Epoch:   74  Gen Loss:      3.6  Dis Loss:      0.3  Total:      3.9  Time: 00:07:39\n",
      "Epoch:   75  Gen Loss:      3.8  Dis Loss:      0.3  Total:      4.1  Time: 00:07:46\n",
      "Epoch:   76  Gen Loss:      4.6  Dis Loss:      0.1  Total:      4.8  Time: 00:07:52\n",
      "Epoch:   77  Gen Loss:      4.8  Dis Loss:      0.1  Total:      4.9  Time: 00:07:58\n",
      "Epoch:   78  Gen Loss:      5.1  Dis Loss:      0.1  Total:      5.2  Time: 00:08:04\n",
      "Epoch:   79  Gen Loss:      4.7  Dis Loss:      0.1  Total:      4.8  Time: 00:08:10\n",
      "Epoch:   80  Gen Loss:      4.6  Dis Loss:      0.1  Total:      4.7  Time: 00:08:17\n",
      "Epoch:   81  Gen Loss:      4.7  Dis Loss:      0.1  Total:      4.8  Time: 00:08:23\n",
      "Epoch:   82  Gen Loss:      4.5  Dis Loss:      0.1  Total:      4.6  Time: 00:08:29\n",
      "Epoch:   83  Gen Loss:      4.7  Dis Loss:      0.1  Total:      4.8  Time: 00:08:35\n",
      "Epoch:   84  Gen Loss:      4.7  Dis Loss:      0.1  Total:      4.8  Time: 00:08:41\n",
      "Epoch:   85  Gen Loss:      5.3  Dis Loss:      0.1  Total:      5.3  Time: 00:08:47\n",
      "Epoch:   86  Gen Loss:      5.2  Dis Loss:      0.1  Total:      5.3  Time: 00:08:53\n",
      "Epoch:   87  Gen Loss:      5.4  Dis Loss:      0.1  Total:      5.4  Time: 00:09:00\n",
      "Epoch:   88  Gen Loss:      5.4  Dis Loss:      0.1  Total:      5.5  Time: 00:09:06\n",
      "Epoch:   89  Gen Loss:      5.4  Dis Loss:      0.1  Total:      5.5  Time: 00:09:12\n",
      "Epoch:   90  Gen Loss:      5.5  Dis Loss:      0.0  Total:      5.6  Time: 00:09:18\n",
      "Epoch:   91  Gen Loss:      5.7  Dis Loss:      0.0  Total:      5.7  Time: 00:09:24\n",
      "Epoch:   92  Gen Loss:      5.8  Dis Loss:      0.0  Total:      5.8  Time: 00:09:30\n",
      "Epoch:   93  Gen Loss:      5.6  Dis Loss:      0.0  Total:      5.7  Time: 00:09:36\n",
      "Epoch:   94  Gen Loss:      5.7  Dis Loss:      0.0  Total:      5.7  Time: 00:09:42\n",
      "Epoch:   95  Gen Loss:      6.0  Dis Loss:      0.0  Total:      6.0  Time: 00:09:49\n",
      "Epoch:   96  Gen Loss:      6.0  Dis Loss:      0.0  Total:      6.0  Time: 00:09:55\n",
      "Epoch:   97  Gen Loss:      6.1  Dis Loss:      0.0  Total:      6.2  Time: 00:10:01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   98  Gen Loss:      6.2  Dis Loss:      0.0  Total:      6.3  Time: 00:10:07\n",
      "Epoch:   99  Gen Loss:      6.4  Dis Loss:      0.0  Total:      6.4  Time: 00:10:13\n",
      "Epoch:  100  Gen Loss:      6.4  Dis Loss:      0.0  Total:      6.4  Time: 00:10:19\n",
      "Epoch:  101  Gen Loss:      6.4  Dis Loss:      0.0  Total:      6.4  Time: 00:10:26\n",
      "Epoch:  102  Gen Loss:      6.2  Dis Loss:      0.0  Total:      6.3  Time: 00:10:32\n",
      "Epoch:  103  Gen Loss:      6.4  Dis Loss:      0.0  Total:      6.4  Time: 00:10:38\n",
      "Epoch:  104  Gen Loss:      6.6  Dis Loss:      0.0  Total:      6.7  Time: 00:10:44\n",
      "Epoch:  105  Gen Loss:      6.6  Dis Loss:      0.0  Total:      6.7  Time: 00:10:50\n",
      "Epoch:  106  Gen Loss:      6.6  Dis Loss:      0.0  Total:      6.6  Time: 00:10:56\n",
      "Epoch:  107  Gen Loss:      6.8  Dis Loss:      0.0  Total:      6.8  Time: 00:11:03\n",
      "Epoch:  108  Gen Loss:      6.8  Dis Loss:      0.0  Total:      6.8  Time: 00:11:09\n",
      "Epoch:  109  Gen Loss:      6.8  Dis Loss:      0.0  Total:      6.8  Time: 00:11:15\n",
      "Epoch:  110  Gen Loss:      6.7  Dis Loss:      0.0  Total:      6.7  Time: 00:11:22\n",
      "Epoch:  111  Gen Loss:      6.8  Dis Loss:      0.0  Total:      6.8  Time: 00:11:28\n",
      "Epoch:  112  Gen Loss:      7.0  Dis Loss:      0.0  Total:      7.0  Time: 00:11:34\n",
      "Epoch:  113  Gen Loss:      7.0  Dis Loss:      0.0  Total:      7.0  Time: 00:11:40\n",
      "Epoch:  114  Gen Loss:      7.1  Dis Loss:      0.0  Total:      7.1  Time: 00:11:47\n",
      "Epoch:  115  Gen Loss:      7.3  Dis Loss:      0.0  Total:      7.3  Time: 00:11:53\n",
      "Epoch:  116  Gen Loss:      7.2  Dis Loss:      0.0  Total:      7.2  Time: 00:11:59\n",
      "Epoch:  117  Gen Loss:      7.2  Dis Loss:      0.0  Total:      7.2  Time: 00:12:05\n",
      "Epoch:  118  Gen Loss:      7.3  Dis Loss:      0.0  Total:      7.3  Time: 00:12:11\n",
      "Epoch:  119  Gen Loss:      7.1  Dis Loss:      0.0  Total:      7.2  Time: 00:12:18\n",
      "Epoch:  120  Gen Loss:      7.3  Dis Loss:      0.0  Total:      7.3  Time: 00:12:24\n",
      "Epoch:  121  Gen Loss:      7.5  Dis Loss:      0.0  Total:      7.5  Time: 00:12:30\n",
      "Epoch:  122  Gen Loss:      7.6  Dis Loss:      0.0  Total:      7.6  Time: 00:12:37\n",
      "Epoch:  123  Gen Loss:      7.6  Dis Loss:      0.0  Total:      7.6  Time: 00:12:43\n",
      "Epoch:  124  Gen Loss:      7.7  Dis Loss:      0.0  Total:      7.7  Time: 00:12:49\n",
      "Epoch:  125  Gen Loss:      7.6  Dis Loss:      0.0  Total:      7.6  Time: 00:12:55\n",
      "Epoch:  126  Gen Loss:      7.8  Dis Loss:      0.0  Total:      7.8  Time: 00:13:01\n",
      "Epoch:  127  Gen Loss:      7.6  Dis Loss:      0.0  Total:      7.6  Time: 00:13:07\n",
      "Epoch:  128  Gen Loss:      7.7  Dis Loss:      0.0  Total:      7.7  Time: 00:13:13\n",
      "Epoch:  129  Gen Loss:      7.7  Dis Loss:      0.0  Total:      7.7  Time: 00:13:20\n",
      "Epoch:  130  Gen Loss:      7.8  Dis Loss:      0.0  Total:      7.8  Time: 00:13:26\n",
      "Epoch:  131  Gen Loss:      7.7  Dis Loss:      0.0  Total:      7.7  Time: 00:13:32\n",
      "Epoch:  132  Gen Loss:      7.9  Dis Loss:      0.0  Total:      7.9  Time: 00:13:38\n",
      "Epoch:  133  Gen Loss:      7.8  Dis Loss:      0.0  Total:      7.8  Time: 00:13:45\n",
      "Epoch:  134  Gen Loss:      7.9  Dis Loss:      0.0  Total:      7.9  Time: 00:13:51\n",
      "Epoch:  135  Gen Loss:      8.3  Dis Loss:      0.0  Total:      8.3  Time: 00:13:57\n",
      "Epoch:  136  Gen Loss:     14.1  Dis Loss:      0.3  Total:     14.4  Time: 00:14:03\n",
      "Epoch:  137  Gen Loss:     10.1  Dis Loss:      0.0  Total:     10.2  Time: 00:14:09\n",
      "Epoch:  138  Gen Loss:      6.5  Dis Loss:      0.2  Total:      6.7  Time: 00:14:15\n",
      "Epoch:  139  Gen Loss:      6.1  Dis Loss:      0.1  Total:      6.2  Time: 00:14:22\n"
     ]
    }
   ],
   "source": [
    "# fix random seed\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "#torch.use_deterministic_algorithms(True)\n",
    "\n",
    "# initialize the model\n",
    "model = Model(\n",
    "    number_of_hidden_layers=number_of_hidden_layers, \n",
    "    latent_size=latent_size, \n",
    "    hidden_size=hidden_size, \n",
    "    output_size=image_size*image_size, \n",
    ").to(device)\n",
    "\n",
    "# log metrics\n",
    "loss_d = np.zeros(epochs)\n",
    "loss_g = np.zeros(epochs)\n",
    "\n",
    "optimizer_discriminator = torch.optim.Adam(model.discriminator.parameters(), lr=learning_rate)\n",
    "optimizer_generator = torch.optim.Adam(model.generator.parameters(), lr=learning_rate)\n",
    "\n",
    "print(\"Starting Training!\")\n",
    "startTime = time.time()\n",
    "for epoch in range(epochs):\n",
    "    total_epoch_size = 0\n",
    "\n",
    "    for batch_idx, batch_data in enumerate(mnist):\n",
    "\n",
    "        x_real, y_real = batch_data\n",
    "        \n",
    "        # flatten input images and move to device\n",
    "        x_real = x_real.to(device)\n",
    "        y_real = y_real.to(device)\n",
    "        n_batch = x_real.shape[0]\n",
    "        x_real = x_real.reshape(n_batch, -1)\n",
    "\n",
    "        # TODO: write your training code here to compute loss functions\n",
    "        #   and use optimizer(s) to update your parameters\n",
    "        loss_discriminator = torch.tensor(0)\n",
    "        loss_generator = torch.tensor(0)\n",
    "\n",
    "         # Train discriminator\n",
    "        optimizer_discriminator.zero_grad()\n",
    "\n",
    "        # Generate fake images\n",
    "        z = torch.randn(n_batch, latent_size).to(device)\n",
    "        fake_labels = Variable(torch.LongTensor(np.random.randint(0, 2, n_batch))).to(device)\n",
    "        x_fake = model.generator(z, fake_labels).to(device)\n",
    "\n",
    "        # Compute discriminator loss for real and fake images\n",
    "        y_real_pred = model.discriminator(x_real, y_real)\n",
    "        y_fake_pred = model.discriminator(x_fake.detach(),fake_labels)\n",
    "\n",
    "        loss_real = discrim_loss_func(y_real_pred, torch.full((n_batch, 1), label_real).to(device))\n",
    "        loss_fake = discrim_loss_func(y_fake_pred, torch.full((n_batch, 1), label_fake).to(device))\n",
    "        loss_discriminator = (loss_real + loss_fake)\n",
    "        \n",
    "        loss_discriminator.backward()\n",
    "        optimizer_discriminator.step()\n",
    "\n",
    "        # Train generator\n",
    "        optimizer_generator.zero_grad()\n",
    "\n",
    "        # Generate fake images again and compute generator loss\n",
    "        y_fake_pred = model.discriminator(x_fake, fake_labels)\n",
    "        loss_generator = discrim_loss_func(y_fake_pred, torch.full((n_batch, 1), label_real).to(device))\n",
    "\n",
    "        loss_generator.backward()\n",
    "        optimizer_generator.step()\n",
    "\n",
    "        # Encode and decode real images\n",
    "        #z_encoded = model.encoder(x_real)\n",
    "        #x_decoded = model.generator(z_encoded)\n",
    "        \n",
    "        # log losses and scores\n",
    "        loss_d[epoch] += loss_discriminator.detach().item() * n_batch\n",
    "        loss_g[epoch] += loss_generator.detach().item() * n_batch\n",
    "        total_epoch_size += n_batch\n",
    "\n",
    "    loss_d[epoch] /= total_epoch_size\n",
    "    loss_g[epoch] /= total_epoch_size\n",
    "\n",
    "    #if epoch == 0 or (epoch + 1) % max(1, epochs // 10) == 0:\n",
    "    total = loss_g[epoch] + loss_d[epoch]\n",
    "    duration = round(time.time() - startTime)\n",
    "    log = \"  \".join([\n",
    "        f\"Epoch: {epoch + 1:4d}\",\n",
    "        f\"Gen Loss: {loss_g[epoch]:8.1f}\",\n",
    "        f\"Dis Loss: {loss_d[epoch]:8.1f}\",\n",
    "        f\"Total: {total:8.1f}\",\n",
    "        f\"Time: {(duration//3600):02.0f}:{((duration%3600)//60):02.0f}:{(duration%60):02.0f}\"\n",
    "    ])\n",
    "    print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6581710e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m nrows, ncols \u001b[39m=\u001b[39m (\u001b[39m3\u001b[39m, \u001b[39m3\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m fig, axes \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots(nrows\u001b[39m=\u001b[39mnrows, ncols\u001b[39m=\u001b[39mncols, figsize\u001b[39m=\u001b[39m(\u001b[39m3\u001b[39m \u001b[39m*\u001b[39m nrows, \u001b[39m3\u001b[39m \u001b[39m*\u001b[39m ncols))\n\u001b[0;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(nrows):\n\u001b[0;32m      5\u001b[0m     \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(ncols):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "nrows, ncols = (3, 3)\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(3 * nrows, 3 * ncols))\n",
    "\n",
    "for row in range(nrows):\n",
    "    for col in range(ncols):\n",
    "        z = torch.randn(1, latent_size).to(device)\n",
    "        fake_labels = Variable(torch.LongTensor(np.random.randint(0, 2, 1))).to(device)\n",
    "        output = model.generator(z,fake_labels).reshape(image_size, image_size).detach().cpu()\n",
    "        axes[row, col].imshow(output.squeeze(), cmap='gray')\n",
    "        axes[row, col].axis('off')\n",
    "\n",
    "#plt.show()\n",
    "\n",
    "for i in range(10):\n",
    "    z = torch.randn(1, latent_size).to(device)\n",
    "    fake_labels = Variable(torch.LongTensor(np.random.randint(0, 1, 1))).to(device)\n",
    "    output = model.generator(z,fake_labels).reshape(image_size, image_size).detach().cpu()\n",
    "    output = output.squeeze()\n",
    "    output.save(os.path.join(sys.path[0], f\"output_data/NORMAL/img{i}.jpeg\"))\n",
    "    \n",
    "\n",
    "for i in range(10):\n",
    "    z = torch.randn(1, latent_size).to(device)\n",
    "    fake_labels = Variable(torch.LongTensor(np.random.randint(1, 2, 1))).to(device)\n",
    "    output = model.generator(z,fake_labels).reshape(image_size, image_size).detach().cpu()\n",
    "    output = output.squeeze()\n",
    "    output.save(os.path.join(sys.path[0], f\"output_data/PNEUMONIA/img{i}.jpeg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50903e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "%python -m pytorch_fid output_data/NORMAL image_data/cleaned_test/NORMAL"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
