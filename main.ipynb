{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d66f185a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#Needed for deterministic behavior\n",
    "#os.environ[\"CUBLAS_WORKSPACE_CONFIG\"]= \":16:8\"\n",
    "#os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "\n",
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2750cbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from pathlib import Path\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "from tqdm import tqdm\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ece07baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPGenerator(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 number_of_hidden_layers: int, \n",
    "                 input_size: int, \n",
    "                 hidden_size: int, \n",
    "                 output_size: int,\n",
    "                 activation: torch.nn.Module):\n",
    "        \"\"\"Construct a simple MLP generator\"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        self.label_emb = torch.nn.Embedding(2,2)\n",
    "        assert number_of_hidden_layers >= 0, \"Generator number_of_hidden_layers must be at least 0\"\n",
    "        \n",
    "        dims_in = [input_size] + [hidden_size] * number_of_hidden_layers\n",
    "        # final output should be the size of a true example\n",
    "        dims_out = [hidden_size] * number_of_hidden_layers + [output_size] \n",
    "        layers = []\n",
    "        for i in range(number_of_hidden_layers + 1):\n",
    "            layers.append(torch.nn.Linear(dims_in[i], dims_out[i]))\n",
    "            \n",
    "            if i < number_of_hidden_layers:\n",
    "                layers.append(activation)\n",
    "        \n",
    "        # apply Sigmoid after final layer to constrain generated images to [0, 1]\n",
    "        layers.append(torch.nn.Sigmoid())\n",
    "        \n",
    "        self.net = torch.nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, labels):\n",
    "        c = self.label_emb(labels)\n",
    "        xx = torch.cat([z,c],1)\n",
    "        return self.net(xx)\n",
    "\n",
    "class MLPDiscriminator(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 number_of_hidden_layers: int, \n",
    "                 input_size: int, \n",
    "                 hidden_size: int, \n",
    "                 activation: torch.nn.Module):\n",
    "        \"\"\"Construct a simple MLP discriminator\"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        self.label_emb = torch.nn.Embedding(2,2)\n",
    "        msg = \"Discriminator number_of_hidden_layers must be at least 0\"\n",
    "        assert number_of_hidden_layers >= 0, msg\n",
    "        \n",
    "        # final output dimension is scalar (probability image is real)\n",
    "        dims_in = [input_size] + [hidden_size] * number_of_hidden_layers\n",
    "        dims_out = [hidden_size] * number_of_hidden_layers + [1]\n",
    "        \n",
    "        layers = []\n",
    "        for i in range(number_of_hidden_layers + 1):\n",
    "            layers.append(torch.nn.Linear(dims_in[i], dims_out[i]))\n",
    "            \n",
    "            if i < number_of_hidden_layers:\n",
    "                layers.append(activation)\n",
    "        \n",
    "        # apply sigmoid after final layer to represent probability\n",
    "        layers.append(torch.nn.Sigmoid())\n",
    "        \n",
    "        self.net = torch.nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, labels):\n",
    "        c = self.label_emb(labels)\n",
    "        xx = torch.cat([x,c],1)\n",
    "        return self.net(xx)\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 number_of_hidden_layers: int, \n",
    "                 latent_size: int, \n",
    "                 hidden_size: int, \n",
    "                 output_size: int,\n",
    "                 activation_generator: torch.nn.Module = torch.nn.ReLU(),\n",
    "                 activation_discriminator: torch.nn.Module = torch.nn.LeakyReLU(0.2)\n",
    "                ):\n",
    "        \n",
    "        super().__init__()\n",
    "                          \n",
    "        self.generator = MLPGenerator(\n",
    "            number_of_hidden_layers=number_of_hidden_layers, \n",
    "            input_size=latent_size+2, \n",
    "            hidden_size=hidden_size, \n",
    "            output_size=output_size,\n",
    "            activation=activation_generator\n",
    "        )\n",
    "        \n",
    "        self.discriminator = MLPDiscriminator(\n",
    "            number_of_hidden_layers=number_of_hidden_layers, \n",
    "            input_size=output_size+2, \n",
    "            hidden_size=hidden_size,\n",
    "            activation=activation_discriminator\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "391d9d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(batch_size: int = 128):\n",
    "    \"\"\"\n",
    "    Load dataset.\n",
    "    :return: DataLoader object\n",
    "    \"\"\"\n",
    "\n",
    "    cuda_kwargs = {\n",
    "        'num_workers': 1,\n",
    "        'pin_memory': True,\n",
    "        'shuffle': True\n",
    "    } if torch.cuda.is_available() else {}\n",
    "\n",
    "    # format image data, but do not normalize\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.Grayscale(num_output_channels=1),\n",
    "         transforms.ToTensor()])\n",
    "    path = \"image_data/cleaned_train\"\n",
    "    dataset = datasets.ImageFolder(path,transform)\n",
    "    # load data\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        **cuda_kwargs\n",
    "    )\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7138e62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is cuda\n"
     ]
    }
   ],
   "source": [
    "# Training hyperparameters that you shouldn't need to change.\n",
    "# Consider changing batch size only if you are trying to speed up your training.\n",
    "image_size = 128 #28\n",
    "batch_size = 64\n",
    "\n",
    "# select device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device is\",device)\n",
    "\n",
    "# load MNIST dataset\n",
    "mnist = load_data(batch_size=batch_size)\n",
    "\n",
    "# discriminator loss function: binary cross-entropy loss\n",
    "discrim_loss_func = torch.nn.BCELoss()\n",
    "\n",
    "# determine which labels will correspond to \"real\" and \"fake\" predictions from the discriminator\n",
    "label_real = 1.0\n",
    "label_fake = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d7acb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Training hyperparameters\n",
    "    hidden_size = 512\n",
    "    number_of_hidden_layers = 4\n",
    "    learning_rate = 0.0001\n",
    "    epochs = 200\n",
    "    latent_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abec419f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training!\n",
      "Epoch:    1  Gen Loss:      2.3  Dis Loss:      1.0  Total:      3.4  Time: 00:00:05\n",
      "Epoch:    2  Gen Loss:      2.1  Dis Loss:      0.9  Total:      3.0  Time: 00:00:09\n",
      "Epoch:    3  Gen Loss:      2.2  Dis Loss:      1.4  Total:      3.6  Time: 00:00:13\n",
      "Epoch:    4  Gen Loss:      3.9  Dis Loss:      0.6  Total:      4.5  Time: 00:00:17\n",
      "Epoch:    5  Gen Loss:      2.0  Dis Loss:      1.4  Total:      3.4  Time: 00:00:21\n",
      "Epoch:    6  Gen Loss:      1.7  Dis Loss:      1.0  Total:      2.7  Time: 00:00:25\n",
      "Epoch:    7  Gen Loss:      1.4  Dis Loss:      0.8  Total:      2.2  Time: 00:00:28\n",
      "Epoch:    8  Gen Loss:      1.2  Dis Loss:      1.2  Total:      2.4  Time: 00:00:32\n",
      "Epoch:    9  Gen Loss:      3.4  Dis Loss:      0.9  Total:      4.3  Time: 00:00:36\n",
      "Epoch:   10  Gen Loss:      8.1  Dis Loss:      0.5  Total:      8.5  Time: 00:00:40\n",
      "Epoch:   11  Gen Loss:     13.1  Dis Loss:      0.7  Total:     13.8  Time: 00:00:44\n",
      "Epoch:   12  Gen Loss:      9.3  Dis Loss:      0.2  Total:      9.5  Time: 00:00:47\n",
      "Epoch:   13  Gen Loss:      7.2  Dis Loss:      0.1  Total:      7.3  Time: 00:00:51\n",
      "Epoch:   14  Gen Loss:      2.3  Dis Loss:      2.8  Total:      5.1  Time: 00:00:55\n",
      "Epoch:   15  Gen Loss:      1.8  Dis Loss:      1.3  Total:      3.0  Time: 00:00:59\n",
      "Epoch:   16  Gen Loss:      1.3  Dis Loss:      1.9  Total:      3.2  Time: 00:01:03\n",
      "Epoch:   17  Gen Loss:      1.8  Dis Loss:      3.0  Total:      4.8  Time: 00:01:07\n",
      "Epoch:   18  Gen Loss:      4.7  Dis Loss:      2.7  Total:      7.4  Time: 00:01:11\n",
      "Epoch:   19  Gen Loss:      8.3  Dis Loss:      3.1  Total:     11.4  Time: 00:01:14\n",
      "Epoch:   20  Gen Loss:      9.5  Dis Loss:      0.9  Total:     10.4  Time: 00:01:18\n",
      "Epoch:   21  Gen Loss:      3.0  Dis Loss:      1.8  Total:      4.8  Time: 00:01:22\n",
      "Epoch:   22  Gen Loss:      2.3  Dis Loss:      1.8  Total:      4.1  Time: 00:01:26\n",
      "Epoch:   23  Gen Loss:      0.8  Dis Loss:      2.2  Total:      3.0  Time: 00:01:30\n",
      "Epoch:   24  Gen Loss:      1.5  Dis Loss:      1.1  Total:      2.5  Time: 00:01:34\n",
      "Epoch:   25  Gen Loss:      2.0  Dis Loss:      0.8  Total:      2.8  Time: 00:01:38\n",
      "Epoch:   26  Gen Loss:      0.7  Dis Loss:      1.8  Total:      2.5  Time: 00:01:42\n",
      "Epoch:   27  Gen Loss:      0.7  Dis Loss:      1.7  Total:      2.4  Time: 00:01:46\n",
      "Epoch:   28  Gen Loss:      1.4  Dis Loss:      1.1  Total:      2.5  Time: 00:01:50\n",
      "Epoch:   29  Gen Loss:      0.7  Dis Loss:      1.5  Total:      2.2  Time: 00:01:54\n",
      "Epoch:   30  Gen Loss:      0.6  Dis Loss:      1.9  Total:      2.6  Time: 00:01:57\n",
      "Epoch:   31  Gen Loss:      1.3  Dis Loss:      0.9  Total:      2.2  Time: 00:02:01\n",
      "Epoch:   32  Gen Loss:      0.9  Dis Loss:      1.3  Total:      2.1  Time: 00:02:05\n",
      "Epoch:   33  Gen Loss:      0.8  Dis Loss:      2.0  Total:      2.8  Time: 00:02:09\n",
      "Epoch:   34  Gen Loss:      1.4  Dis Loss:      1.3  Total:      2.6  Time: 00:02:13\n",
      "Epoch:   35  Gen Loss:      0.8  Dis Loss:      1.4  Total:      2.2  Time: 00:02:17\n",
      "Epoch:   36  Gen Loss:      1.0  Dis Loss:      1.4  Total:      2.4  Time: 00:02:21\n",
      "Epoch:   37  Gen Loss:      0.8  Dis Loss:      1.4  Total:      2.2  Time: 00:02:25\n",
      "Epoch:   38  Gen Loss:      0.9  Dis Loss:      1.2  Total:      2.0  Time: 00:02:29\n",
      "Epoch:   39  Gen Loss:      0.9  Dis Loss:      1.4  Total:      2.3  Time: 00:02:33\n",
      "Epoch:   40  Gen Loss:      1.0  Dis Loss:      1.3  Total:      2.3  Time: 00:02:37\n",
      "Epoch:   41  Gen Loss:      0.8  Dis Loss:      1.6  Total:      2.4  Time: 00:02:41\n",
      "Epoch:   42  Gen Loss:      0.8  Dis Loss:      1.4  Total:      2.1  Time: 00:02:45\n",
      "Epoch:   43  Gen Loss:      0.8  Dis Loss:      1.2  Total:      2.0  Time: 00:02:49\n",
      "Epoch:   44  Gen Loss:      0.9  Dis Loss:      1.3  Total:      2.2  Time: 00:02:53\n",
      "Epoch:   45  Gen Loss:      0.7  Dis Loss:      1.5  Total:      2.2  Time: 00:02:57\n",
      "Epoch:   46  Gen Loss:      1.2  Dis Loss:      1.0  Total:      2.2  Time: 00:03:01\n",
      "Epoch:   47  Gen Loss:      0.7  Dis Loss:      1.7  Total:      2.5  Time: 00:03:04\n",
      "Epoch:   48  Gen Loss:      0.9  Dis Loss:      1.3  Total:      2.2  Time: 00:03:08\n",
      "Epoch:   49  Gen Loss:      0.8  Dis Loss:      1.5  Total:      2.2  Time: 00:03:12\n",
      "Epoch:   50  Gen Loss:      0.9  Dis Loss:      1.1  Total:      2.0  Time: 00:03:16\n",
      "Epoch:   51  Gen Loss:      0.7  Dis Loss:      1.6  Total:      2.3  Time: 00:03:20\n",
      "Epoch:   52  Gen Loss:      0.9  Dis Loss:      1.3  Total:      2.2  Time: 00:03:24\n",
      "Epoch:   53  Gen Loss:      0.8  Dis Loss:      1.8  Total:      2.6  Time: 00:03:27\n",
      "Epoch:   54  Gen Loss:      1.2  Dis Loss:      1.1  Total:      2.3  Time: 00:03:31\n",
      "Epoch:   55  Gen Loss:      0.8  Dis Loss:      1.4  Total:      2.2  Time: 00:03:35\n",
      "Epoch:   56  Gen Loss:      1.4  Dis Loss:      1.1  Total:      2.5  Time: 00:03:39\n",
      "Epoch:   57  Gen Loss:      1.5  Dis Loss:      1.3  Total:      2.8  Time: 00:03:43\n",
      "Epoch:   58  Gen Loss:      3.0  Dis Loss:      1.1  Total:      4.0  Time: 00:03:47\n",
      "Epoch:   59  Gen Loss:      3.5  Dis Loss:      2.1  Total:      5.6  Time: 00:03:51\n",
      "Epoch:   60  Gen Loss:      0.9  Dis Loss:      2.4  Total:      3.4  Time: 00:03:55\n",
      "Epoch:   61  Gen Loss:      3.7  Dis Loss:      1.5  Total:      5.2  Time: 00:03:59\n",
      "Epoch:   62  Gen Loss:     11.3  Dis Loss:      2.3  Total:     13.6  Time: 00:04:03\n",
      "Epoch:   63  Gen Loss:     33.2  Dis Loss:     13.4  Total:     46.7  Time: 00:04:07\n",
      "Epoch:   64  Gen Loss:      0.0  Dis Loss:    100.0  Total:    100.0  Time: 00:04:11\n",
      "Epoch:   65  Gen Loss:      0.0  Dis Loss:    100.0  Total:    100.0  Time: 00:04:14\n",
      "Epoch:   66  Gen Loss:      0.0  Dis Loss:    100.0  Total:    100.0  Time: 00:04:18\n",
      "Epoch:   67  Gen Loss:      0.0  Dis Loss:    100.0  Total:    100.0  Time: 00:04:22\n",
      "Epoch:   68  Gen Loss:      0.0  Dis Loss:    100.0  Total:    100.0  Time: 00:04:26\n",
      "Epoch:   69  Gen Loss:      0.0  Dis Loss:    100.0  Total:    100.0  Time: 00:04:30\n",
      "Epoch:   70  Gen Loss:      0.0  Dis Loss:    100.0  Total:    100.0  Time: 00:04:34\n",
      "Epoch:   71  Gen Loss:      0.0  Dis Loss:    100.0  Total:    100.0  Time: 00:04:38\n",
      "Epoch:   72  Gen Loss:      0.0  Dis Loss:    100.0  Total:    100.0  Time: 00:04:42\n",
      "Epoch:   73  Gen Loss:      0.0  Dis Loss:    100.0  Total:    100.0  Time: 00:04:46\n",
      "Epoch:   74  Gen Loss:      0.0  Dis Loss:    100.0  Total:    100.0  Time: 00:04:50\n",
      "Epoch:   75  Gen Loss:      0.0  Dis Loss:    100.0  Total:    100.0  Time: 00:04:54\n",
      "Epoch:   76  Gen Loss:      0.0  Dis Loss:    100.0  Total:    100.0  Time: 00:04:58\n",
      "Epoch:   77  Gen Loss:      0.0  Dis Loss:    100.0  Total:    100.0  Time: 00:05:02\n",
      "Epoch:   78  Gen Loss:      0.0  Dis Loss:    100.0  Total:    100.0  Time: 00:05:06\n",
      "Epoch:   79  Gen Loss:      0.0  Dis Loss:    100.0  Total:    100.0  Time: 00:05:10\n"
     ]
    }
   ],
   "source": [
    "# fix random seed\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "#torch.use_deterministic_algorithms(True)\n",
    "\n",
    "# initialize the model\n",
    "model = Model(\n",
    "    number_of_hidden_layers=number_of_hidden_layers, \n",
    "    latent_size=latent_size, \n",
    "    hidden_size=hidden_size, \n",
    "    output_size=image_size*image_size, \n",
    ").to(device)\n",
    "\n",
    "# log metrics\n",
    "loss_d = np.zeros(epochs)\n",
    "loss_g = np.zeros(epochs)\n",
    "\n",
    "optimizer_discriminator = torch.optim.Adam(model.discriminator.parameters(), lr=learning_rate)\n",
    "optimizer_generator = torch.optim.Adam(model.generator.parameters(), lr=learning_rate)\n",
    "\n",
    "print(\"Starting Training!\")\n",
    "startTime = time.time()\n",
    "for epoch in range(epochs):\n",
    "    total_epoch_size = 0\n",
    "\n",
    "    for batch_idx, batch_data in enumerate(mnist):\n",
    "\n",
    "        x_real, y_real = batch_data\n",
    "        \n",
    "        # flatten input images and move to device\n",
    "        x_real = x_real.to(device)\n",
    "        y_real = y_real.to(device)\n",
    "        n_batch = x_real.shape[0]\n",
    "        x_real = x_real.reshape(n_batch, -1)\n",
    "\n",
    "        # TODO: write your training code here to compute loss functions\n",
    "        #   and use optimizer(s) to update your parameters\n",
    "        loss_discriminator = torch.tensor(0)\n",
    "        loss_generator = torch.tensor(0)\n",
    "\n",
    "         # Train discriminator\n",
    "        optimizer_discriminator.zero_grad()\n",
    "\n",
    "        # Generate fake images\n",
    "        z = torch.randn(n_batch, latent_size).to(device)\n",
    "        fake_labels = Variable(torch.LongTensor(np.random.randint(0, 2, n_batch))).to(device)\n",
    "        x_fake = model.generator(z, fake_labels).to(device)\n",
    "\n",
    "        # Compute discriminator loss for real and fake images\n",
    "        y_real_pred = model.discriminator(x_real, y_real)\n",
    "        y_fake_pred = model.discriminator(x_fake.detach(),fake_labels)\n",
    "\n",
    "        loss_real = discrim_loss_func(y_real_pred, torch.full((n_batch, 1), label_real).to(device))\n",
    "        loss_fake = discrim_loss_func(y_fake_pred, torch.full((n_batch, 1), label_fake).to(device))\n",
    "        loss_discriminator = (loss_real + loss_fake)\n",
    "        \n",
    "        loss_discriminator.backward()\n",
    "        optimizer_discriminator.step()\n",
    "\n",
    "        # Train generator\n",
    "        optimizer_generator.zero_grad()\n",
    "\n",
    "        # Generate fake images again and compute generator loss\n",
    "        y_fake_pred = model.discriminator(x_fake, fake_labels)\n",
    "        loss_generator = discrim_loss_func(y_fake_pred, torch.full((n_batch, 1), label_real).to(device))\n",
    "\n",
    "        loss_generator.backward()\n",
    "        optimizer_generator.step()\n",
    "\n",
    "        # Encode and decode real images\n",
    "        #z_encoded = model.encoder(x_real)\n",
    "        #x_decoded = model.generator(z_encoded)\n",
    "        \n",
    "        # log losses and scores\n",
    "        loss_d[epoch] += loss_discriminator.detach().item() * n_batch\n",
    "        loss_g[epoch] += loss_generator.detach().item() * n_batch\n",
    "        total_epoch_size += n_batch\n",
    "\n",
    "    loss_d[epoch] /= total_epoch_size\n",
    "    loss_g[epoch] /= total_epoch_size\n",
    "\n",
    "    #if epoch == 0 or (epoch + 1) % max(1, epochs // 10) == 0:\n",
    "    total = loss_g[epoch] + loss_d[epoch]\n",
    "    duration = round(time.time() - startTime)\n",
    "    log = \"  \".join([\n",
    "        f\"Epoch: {epoch + 1:4d}\",\n",
    "        f\"Gen Loss: {loss_g[epoch]:8.1f}\",\n",
    "        f\"Dis Loss: {loss_d[epoch]:8.1f}\",\n",
    "        f\"Total: {total:8.1f}\",\n",
    "        f\"Time: {(duration//3600):02.0f}:{((duration%3600)//60):02.0f}:{(duration%60):02.0f}\"\n",
    "    ])\n",
    "    print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6581710e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nrows, ncols = (3, 3)\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(3 * nrows, 3 * ncols))\n",
    "\n",
    "for row in range(nrows):\n",
    "    for col in range(ncols):\n",
    "        z = torch.randn(1, latent_size).to(device)\n",
    "        output = model.generator(z).reshape(image_size, image_size).detach().cpu()\n",
    "        axes[row, col].imshow(output.squeeze(), cmap='gray')\n",
    "        axes[row, col].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50903e78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
